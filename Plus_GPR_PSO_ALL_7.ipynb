{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plus_GPR_PSO_ALL_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NZmjEU4mZv03sXB2hv63ZNDWc-ycLgQR",
      "authorship_tag": "ABX9TyPuQjTNgZkNTSAvUONDwu8J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekourkchi/Weather_research/blob/master/Plus_GPR_PSO_ALL_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OnIjP9PbCF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bfe961-f0ab-494c-8d15-341076ea9ff1"
      },
      "source": [
        "!pip install george"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting george\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/19/9de575be629e3a41c3ca6b1e2c80e0ae90a2e831436c5f70cc8d72e37ab7/george-0.3.1.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from george) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from george) (1.4.1)\n",
            "Collecting pybind11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/84/fc9dc13ee536ba5e6b8fd10ce368fea5b738fe394c3b296cde7c9b144a92/pybind11-2.6.1-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 35.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: george\n",
            "  Building wheel for george (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for george\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for george\n",
            "Failed to build george\n",
            "Installing collected packages: pybind11, george\n",
            "    Running setup.py install for george ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed george-0.3.1 pybind11-2.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kslhxOQUpAWi",
        "outputId": "2a9fdb06-1738-4f10-c8de-f9232f65931c"
      },
      "source": [
        "!pip install pyswarms"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyswarms\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/22/ffdc92e71bbfcf6048644df6c428c639a9597abd262233199a37ba255603/pyswarms-1.2.0-py2.py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 19.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from pyswarms) (3.2.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from pyswarms) (20.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from pyswarms) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyswarms) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyswarms) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyswarms) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pyswarms) (4.41.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->pyswarms) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->pyswarms) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->pyswarms) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->pyswarms) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=1.3.1->pyswarms) (1.15.0)\n",
            "Installing collected packages: pyswarms\n",
            "Successfully installed pyswarms-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJPo4yPabY6m"
      },
      "source": [
        "\n",
        "# Importing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwWf_zgebWs0"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from astropy.table import Table, Column \n",
        "from scipy.stats import linregress\n",
        "from scipy import interpolate\n",
        "from scipy import polyval, polyfit\n",
        "from scipy import odr\n",
        "import pylab as py\n",
        "from matplotlib import gridspec\n",
        "import sklearn.datasets as ds\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.optimize as op\n",
        "from scipy.linalg import cholesky, inv,det\n",
        "from scipy.optimize import minimize\n",
        "import george\n",
        "from george import kernels\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIHt3aWgqMsY"
      },
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "from pyswarms.single.global_best import GlobalBestPSO\n",
        "# Import PySwarms\n",
        "import pyswarms as ps\n",
        "from pyswarms.utils.functions import single_obj as fx\n",
        "\n",
        "# Some more magic so that the notebook will reload external python modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_EA5Fa3jEme"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def GPR(X, y, lnlikelihood=True):\n",
        "  '''\n",
        "  The output of this function is another function, either the lnlikelihood, or \n",
        "  the gp (the gaussian process regressor that is dfined by giving theta)\n",
        "  '''\n",
        "  n = X.shape[1]\n",
        "    \n",
        "  def step(theta):\n",
        "\n",
        "        L = np.exp(theta[:n])\n",
        "        sigma = np.exp(theta[n])   \n",
        "        yerr = np.exp(theta[n+1])\n",
        "        \n",
        "        kernel = sigma * kernels.ExpSquaredKernel(np.ones(n), ndim=n)\n",
        "\n",
        "        gp = george.GP(kernel)\n",
        "\n",
        "        if lnlikelihood:\n",
        "          gp = george.GP(kernel)\n",
        "          gp.compute(X / np.vstack([L]*X.shape[0]), yerr)\n",
        "       \n",
        "          return -gp.lnlikelihood(y)\n",
        "        else:\n",
        "          X0 = X / np.vstack([L]*X.shape[0])\n",
        "          gp.compute(X0, yerr)\n",
        "          return gp\n",
        "      \n",
        "  return step"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb37opfcgjKO"
      },
      "source": [
        "def metrics(y1, y2, file=None):\n",
        "  '''\n",
        "  y1 and y2 are two series of the same size\n",
        "\n",
        "  This function outputs the MAE, RMSE and R^2 \n",
        "  of the cross evaluated series.\n",
        "\n",
        "  '''\n",
        "  y1 = y1.reshape(-1)\n",
        "  y2 = y2.reshape(-1)\n",
        "  RMSE = np.sqrt(np.mean((y1-y2)**2))\n",
        "  MAE = np.mean(np.abs(y1-y2))\n",
        "  R2 = r2_score(y1, y2)\n",
        "\n",
        "  if file is None:\n",
        "    print('MAE: %.2f'%MAE, ' RMSE: %.2f'%RMSE, ' R^2: %.2f'%R2)\n",
        "  else:\n",
        "    file.write('MAE: %.2f'%MAE + ' RMSE: %.2f'%RMSE + ' R^2: %.2f'%R2 +'\\n')\n",
        "########################################\n",
        "\n",
        "def funcMAX(func, X, y, addParam = 0, maxiter=500, method='L-BFGS-B', verbose=False):\n",
        "  \n",
        "  '''\n",
        "  A function to find the optimum parameters of the input funtion \"func\",\n",
        "  where yp = func(X) and RMSE(y-yp) is minimzed\n",
        "\n",
        "  output: \"results\" is the object that contains everything about the fit\n",
        "  result.x holds the optimized parameters\n",
        "  '''\n",
        "\n",
        "  t1 =  datetime.now()  # t1 and t2 are used for timing this process\n",
        "  ###########################################\n",
        "  n = X.shape[1]\n",
        "  # Maximum Likelihood\n",
        "  Param_init = np.random.rand(n+addParam)\n",
        "  result = minimize(func(X, y), Param_init, \n",
        "                method=method, options={\"maxiter\":maxiter})\n",
        "  print(\"--------------------\")\n",
        "  if verbose:\n",
        "    print(result)\n",
        "  ###########################################\n",
        "  if not verbose: \n",
        "    print(\"Fit Success: \", result.success)\n",
        "  t2 =  datetime.now()\n",
        "  print(\"Execution time: \", t2-t1)\n",
        "  print(\"--------------------\")\n",
        "\n",
        "  return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3cYrUmmgnK0"
      },
      "source": [
        "def dataPrepare2(y, z, v, w, n = 3, d = 1):\n",
        "  '''\n",
        "  Generating discrete data points out of the given series\n",
        "\n",
        "  y: main signal, e.g. ET0\n",
        "  z, v, w: auxiliary signals (set these to zero (e.g z=y*0) if not interested)\n",
        "  n: the number of previous data points that are used for forcasting\n",
        "  d: the number of days ahead for forecasting\n",
        "\n",
        "  output: feature matrix XS, and target values ys\n",
        "  '''\n",
        "\n",
        "  N = len(y)\n",
        "  dd = d - 1\n",
        "\n",
        "  XS = np.zeros((N-n-dd, 4*n))\n",
        "  ys = np.zeros(N-n-dd)\n",
        "\n",
        "  for i in range(0, N-n-dd):\n",
        "    XS[i,:n] = y[i:i+n]  \n",
        "    XS[i,n:2*n] = v[i:i+n]\n",
        "    XS[i,2*n:3*n] = w[i:i+n]   \n",
        "    XS[i,3*n:4*n] = z[i:i+n] \n",
        "    \n",
        "    ys[i] = y[i+n+dd]\n",
        "  # \n",
        "  return XS, ys"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XABFRiJzbnlu"
      },
      "source": [
        "# Load data\n",
        "data preparation\n",
        "We generate the first 3 main principal components that capture the most useful information of the data. P1, P2 and P3 are not correalted with each other while they are epressed as the linear cominination of the available featurdes, i.e. ET0, VPD, Rn and T (air temperature)\n",
        "\n",
        "**Note:** Make sure that the data file is addressed correctly and it's already avaialble in your Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSb--yh_FdpS"
      },
      "source": [
        "def prepData_IR(station, mDelay, nAhead):\n",
        "\n",
        "  data = pd.read_excel('/content/drive/MyDrive/NWdata.xlsx')\n",
        "\n",
        "  data['TIMESTAMP'] = pd.date_range('20000101', periods=len(data), freq='D')\n",
        "  data.set_index(\"TIMESTAMP\", inplace=True)\n",
        "\n",
        "  # resample data daily, forward linear interpolation to fill the missing values\n",
        "  data.resample('1d').mean()\n",
        "  data = data.interpolate(method='linear', limit_direction='forward', axis=0)\n",
        "\n",
        "  subData = data.loc[\"2000\":\"2006-12-31\"]\n",
        "  N = len(subData)\n",
        "  x = subData.index\n",
        "  y = subData[station].ffill()\n",
        "  z = y*0.\n",
        "  v = y*0.\n",
        "  w = y*0.\n",
        "\n",
        "\n",
        "  # testing: 2013 onward\n",
        "  subData_test = data.loc[\"2007\":]\n",
        "  N_test = len(subData_test)\n",
        "  x_test = subData_test.index\n",
        "  y_test = subData_test[station].ffill()\n",
        "  z_test = y_test*0.\n",
        "  v_test = y_test*0.\n",
        "  w_test = y_test*0.\n",
        "\n",
        "\n",
        "  XS2, ys2 = dataPrepare2(y, z, v, w, n=mDelay, d=nAhead)\n",
        "  XS_test2, ys_test2 = dataPrepare2(y_test, z_test, v_test, w_test, n=mDelay, d=nAhead)\n",
        "\n",
        "  return XS2, ys2, XS_test2, ys_test2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtnfvqCtbgg6"
      },
      "source": [
        "def prepData(station, mDelay, nAhead):\n",
        "\n",
        "        dataFile = station + '.xlsx'  \n",
        "        data = pd.read_excel('/content/drive/My Drive/'+ dataFile)\n",
        "        ######################################################################\n",
        "\n",
        "        # revising the column names\n",
        "        for col in data.columns:\n",
        "          newcol = col.split(\"(\")[0].strip()\n",
        "          data.rename(columns={col:newcol}, inplace=True)\n",
        "\n",
        "        # setting up the index of the data frame\n",
        "        data.set_index(\"TIMESTAMP\", inplace=True)\n",
        "\n",
        "        # resample data daily, forward linear interpolation to fill the missing values\n",
        "        data.resample('1d').mean()\n",
        "        data = data.interpolate(method='linear', limit_direction='forward', axis=0)\n",
        "\n",
        "        ## Optional:\n",
        "        ## Generating the first three principal (P1, P2, P3) components basesd on ET0, VPD, Rn and T.\n",
        "        myData = data[[\"ET0\", \"VPD\", \"Rn\", \"T\"]].ffill()\n",
        "        z_scaler = StandardScaler()\n",
        "        z_data = z_scaler.fit_transform(myData)\n",
        "        pca_data = PCA().fit_transform(z_data);\n",
        "        pca_trafo = PCA().fit(z_data);\n",
        "        data[\"P1\"] = pca_data[:,0]\n",
        "        data[\"P2\"] = pca_data[:,1]\n",
        "        data[\"P3\"] = pca_data[:,3]\n",
        "        ######################################################################\n",
        "\n",
        "        subData = data.loc[\"2010\":\"2016-12-31\"]\n",
        "        N = len(subData)\n",
        "        x = subData.index\n",
        "        y = subData[\"ET0\"].ffill()\n",
        "        z = subData[\"VPD\"].ffill()\n",
        "        v = subData[\"Rn\"].ffill()\n",
        "        w = subData[\"T\"].ffill()\n",
        "\n",
        "\n",
        "        # testing: 2013 onward\n",
        "        subData_test = data.loc[\"2017\":]\n",
        "        N_test = len(subData_test)\n",
        "        x_test = subData_test.index\n",
        "        y_test = subData_test[\"ET0\"].ffill()\n",
        "        z_test = subData_test[\"VPD\"].ffill()\n",
        "        v_test = subData_test[\"Rn\"].ffill()\n",
        "        w_test = subData_test[\"T\"].ffill()\n",
        "\n",
        "\n",
        "        # mDelay = 5\n",
        "        # nAhead = 7\n",
        "\n",
        "        XS2, ys2 = dataPrepare2(y, z, v, w, n=mDelay, d=nAhead)\n",
        "        XS_test2, ys_test2 = dataPrepare2(y_test, z_test, v_test, w_test, n=mDelay, d=nAhead)\n",
        "\n",
        "        return XS2, ys2, XS_test2, ys_test2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrP7M15gHMk-"
      },
      "source": [
        "# ML class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGp8gMljG-Wb"
      },
      "source": [
        "class psoGPR:\n",
        "\n",
        "  def __init__(self, station, mDelay=1, nAhead=1, IR=False):\n",
        "    \n",
        "    self.station = station\n",
        "    self.mDelay = mDelay\n",
        "    self.nAhead = nAhead\n",
        "\n",
        "    if not IR:\n",
        "      self.XS2, self.ys2, self.XS_test2, self.ys_test2 = prepData(station, mDelay, nAhead)\n",
        "    else:\n",
        "      self.XS2, self.ys2, self.XS_test2, self.ys_test2 = prepData_IR(station, mDelay, nAhead)\n",
        "\n",
        "    self.theta = None\n",
        "    self.kf = None\n",
        "\n",
        "\n",
        "  def Xi2_swarm(self, x):\n",
        "    \n",
        "    nParticle = x.shape[0]\n",
        "    out = np.zeros(nParticle)\n",
        "\n",
        "    for train_index, cross_index in self.kf.split(self.XS2):\n",
        "      \n",
        "        X_train, X_cross = self.XS2[train_index], self.XS2[cross_index]\n",
        "        y_train, y_cross = self.ys2[train_index], self.ys2[cross_index]\n",
        "\n",
        "        n = X_cross.shape[1]\n",
        "        m = X_cross.shape[0]\n",
        "\n",
        "        for n_iter in range(nParticle):\n",
        "            \n",
        "            theta = x[n_iter,:]\n",
        "\n",
        "            L = np.exp(theta[:n])\n",
        "\n",
        "            gp = GPR(X_train, y_train, lnlikelihood=False)(theta)\n",
        "            gp_yp_cross, gp_yp_cross_std = gp.predict(y_train, X_cross/np.vstack([L]*m), return_var=True)\n",
        "\n",
        "            out[n_iter] += np.sum((y_cross - gp_yp_cross)**2)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "  def runSwarm(self, kFold=5, n_particles=10, n_iter=10, options={'c1': 0.5, 'c2': 0.3, 'w':0.9}):\n",
        "\n",
        "    self.kf = KFold(n_splits=kFold)\n",
        "    self.kf.get_n_splits(self.XS2)\n",
        "\n",
        "    n_components = 4 * self.mDelay\n",
        "\n",
        "    # Call instance of PSO\n",
        "    optimizer = GlobalBestPSO(n_particles=n_particles, dimensions=n_components+2, options=options)\n",
        "\n",
        "    # Perform optimization\n",
        "    cost, theta = optimizer.optimize(self.Xi2_swarm, iters=n_iter, verbose=False)\n",
        "\n",
        "    self.theta = theta\n",
        "\n",
        "    return cost, theta\n",
        "\n",
        "    \n",
        "  def predict(self):\n",
        "\n",
        "    if self.theta is None:\n",
        "      return None\n",
        "\n",
        "    truths = self.theta\n",
        "    \n",
        "    gp = GPR(self.XS2, self.ys2, lnlikelihood=False)(truths)\n",
        "\n",
        "    n = self.XS2.shape[1]\n",
        "    m = self.XS2.shape[0]\n",
        "\n",
        "    m_test = self.XS_test2.shape[0]\n",
        "\n",
        "    L = np.exp(truths[:n])\n",
        "\n",
        "    gp_yp, gp_yp_std = gp.predict(self.ys2, self.XS2/np.vstack([L]*m), return_var=True)\n",
        "    gp_yp_test, gp_yp_test_std = gp.predict(self.ys2, self.XS_test2/np.vstack([L]*m_test), return_var=True)\n",
        "\n",
        "    return gp_yp, gp_yp_std, gp_yp_test, gp_yp_test_std\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wPkIGjdG-Y4"
      },
      "source": [
        "def runMODEL(station, mDelay = 5, nAhead = 1, kFold = 5, n_particles=2, n_iter=2, IR=False):\n",
        "\n",
        "  # station = 'Changde'\n",
        "  # mDelay = 1\n",
        "  # nAhead = 1\n",
        "  # kFold = 5\n",
        "  # n_particles=2\n",
        "  # n_iter=2\n",
        "  options={'c1': 0.5, 'c2': 0.3, 'w':0.9}\n",
        "\n",
        "  outFile = '/content/drive/My Drive/Weather_res_plus/Plus_'+ station + '_mD%d'%mDelay + '_nA%d'%nAhead\n",
        "\n",
        "  model = psoGPR(station, mDelay=mDelay, nAhead=nAhead, IR=IR)\n",
        "  model.runSwarm(kFold=kFold, n_particles=n_particles, n_iter=n_iter, options=options)\n",
        "  gp_yp, gp_yp_std, gp_yp_test, gp_yp_test_std = model.predict()\n",
        "\n",
        "  train = {'train':model.ys2, 'predict':gp_yp, 'stdev':gp_yp_std}\n",
        "  pd.DataFrame.from_dict(train).to_csv(outFile+'_train.csv')\n",
        "\n",
        "  test = {'test':model.ys_test2, 'predict':gp_yp_test, 'stdev':gp_yp_test_std}\n",
        "  pd.DataFrame.from_dict(test).to_csv(outFile+'_test.csv')\n",
        "\n",
        "  ##############################################################################\n",
        "  with open(outFile+'_metrics.txt', 'w') as file:\n",
        "\n",
        "    file.write(\"station = \" + station + '\\n') \n",
        "    file.write(\"mDelay = %d\\n\"% mDelay) \n",
        "    file.write(\"nAhead = %d\\n\"% nAhead) \n",
        "    file.write(\"kFold = %d\\n\"% kFold) \n",
        "    file.write(\"n_particles = %d\\n\"% n_particles) \n",
        "    file.write(\"n_iter = %d\\n\"% n_iter) \n",
        "\n",
        "    file.write(\"\\n\")\n",
        "\n",
        "    file.write(\"Training set: \"+'\\n')\n",
        "    metrics(model.ys2, gp_yp, file=file)\n",
        "    file.write(\"----------------------\"+'\\n')\n",
        "    file.write(\"Test set: \"+'\\n')\n",
        "    metrics(model.ys_test2, gp_yp_test, file=file)\n",
        "\n",
        "    file.write(\"\\n\\n\")\n",
        "    file.write(\"Model Hyperparameters: \\n\")\n",
        "\n",
        "    for theta in model.theta:\n",
        "      file.write(str(theta)+'\\n')\n",
        "\n",
        "  \n",
        "  print(\"Done. Station: \", station)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN1qTOHu-OPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea84b88-cd21-4903-ebcf-23ed978d6985"
      },
      "source": [
        "STATIONS = [\"Changde\", \"Hailisu\", \"Lancang\", \"Miyun\", \"Nanmulin\", \"Qiemo\"]\n",
        "\n",
        "for station in STATIONS:\n",
        "  runMODEL(station, mDelay = 5, nAhead = 7, kFold = 10, n_particles=20, n_iter=50)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done. Station:  Changde\n",
            "Done. Station:  Hailisu\n",
            "Done. Station:  Lancang\n",
            "Done. Station:  Miyun\n",
            "Done. Station:  Nanmulin\n",
            "Done. Station:  Qiemo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqDFsRg8duge"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}